# TAADA_Analyses

This repository contains the analyses for two studies conducted using TAADA and reported in an under review paper. Data is available for the first study, but not for the second, so users cannot replicate it. R scripts for both analyses are provided along with outputs. 

Included in the repository are

1.  TAADA data extracted from the CLEAR corpus (taada_clear_results_token_cw.csv)
2.  Final data used in the CLEAR analysis (multicollear_ease_reading_taada.csv)
3.  The R notebook for the CLEAR analysis (taada_clear_analysis.Rmd)
4.  The data wrangling and analysis in an R notebook for the miscue study (lrd_rcv_analyses_data_wrangling_analysis_taada_for_github.Rmd)


The two studies are as follows

## Study 1

Our first study is corpus-based and relies on a large-scale collection of text excerpts which each have associated human judgments of readability. The goal of the study is to assess relationships between the TAADA decoding features and the readability judgments.

### Methods

#### Corpus. 
To assess the links between morphological elements of texts and text readability, we used the CommonLit Ease of Readability (CLEAR) corpus (Crossley et al., 2022). The corpus consists of 4,724 text excerpts, which comprise around 800,000 words. The corpus was created for the purpose of developing and evaluating different readability formulas. To obtain distinct readability assessments for the text excerpts, teachers were recruited from CommonLit's teacher network. Teachers were asked to review pairs of text samples and judge which excerpt was simpler for students to understand. Following the exclusion of outliers, contributions from 1,116 educators were retained, amounting to 111,347 comparative assessments in total. A Bradley-Terry model (Bradley & Terry, 1952) was used to compute pairwise comparison scores for the teachers’ judgments of text ease to calculate unique readability scores for each excerpt. The final scores reflect the “Easiness” in terms of comprehension for each excerpt in the corpus. The scoring method and the extraction of readability scores is discussed in Crossley et al. (2022).

#### Statistical Analyses. 
To predict text reading ease scores found in the CLEAR corpus, we used the decoding indices in TAADA as predictor variables in a linear model. We first ensured that none of the TAADA variables correlated strongly with text length (r > .699) and also calculated bivariate Pearson correlations for all TAADA variables using the cor.test() function in R (R Core Team, 2020) to identify highly collinear features among the TAADA variables. If two or more variables correlated at r > .699, the variable(s) with the lowest correlation with the ease of readability score was removed and the variable with the higher correlation was retained. We also only retained variables that demonstrated at least a small relationship with the ease of readability scores (r > .099).
	We used the CARET package (Kuhn, 2008) in R to develop linear models. Model training and evaluation were performed using a ten-fold cross-validation model using stepwise selection from the leapSeq() function. Estimates of accuracy are reported using the amount of variance explained by the developed models (R2). The model was checked for suppression effects. The relative importance of the indices in each model was calculated using the calc.relimp() function in the relaimpo package (Grömping, 2006) using the lmg metric (Lindeman et al., 1980). lmg takes into account both the direct relationship between the independent and dependent variable (i.e., the bivariate correlation) and the indirect relationship between the independent and dependent variable (i.e., the amount of variance explained when included in a multivariate model).


## Study 2

While our first study examined subjective judgments of readability, our second study analyzes more objective measurements of decoding. The data comes from younger readers tasked with reading a text aloud. The read alouds were recorded and each word produced by the students was coded as being accurate or inaccurately pronounced (i.e., miscues). Miscues were coded as words where a child made a deletion, omission, mispronunciation, substitution or self-correction when reading. 

### Datasets

The full dataset used in the second study was extracted from three different read aloud studies conducted in the same lab using the same protocols. Across all three studies, there were 296 total participants who produced 653 distinct words comprising 534 distinct lemmas. In total, there were 120,822 observations of students reading a word aloud, of which participants had no miscues for 114,374 observations and had a miscue for 6,475 observations. 

The goal of the first study was to identify behavioral and neuronal weaknesses in children that have late-emerging reading difficulties. There were 77 participants in this dataset, who were between the ages of 6-8 (M = 7.51, SD = 0.32) when data was collected. The sample was 45.45% male, and their reported race was 62.34% White, 31.17% Black/African American, 5.19% more than one race, and 1.30% Native American. Four passages in this study were taken from the Qualitative Reading Inventory (QRI; Caldwell & Leslie, 2000), which tasked participants with reading passages aloud and answering questions related to the passages to measure word identification, fluency, and comprehension. There were two expository and two narrative passages in total, although participants were counterbalanced to read one narrative and one expository passage out of the four. The first narrative passage was about a mouse in a house, which contained 250 words, and the second was about a surprise gift which contained 210 words. The first expository passage contained 76 words and was about the brain and the five senses. The second was about air and contained 85 words.

The goal of the second study was to understand the role executive functioning plays across reading comprehension development. There were 77 participants in this dataset who were between the ages of 7-9 (M=8.39, SD = 0.34). The sample was 41.56% male students and reported their race as: 81.82% White, 12.99% Black/African American, 5.20% Asian, 3.90% more than one race, and 2.60% preferred not to answer. These four passages were experimenter-created and controlled to be as identical as possible based on number of words, sentence length, word length, word frequency, word concreteness, reading ease, and grade level (Del Tufo, Earle, & Cutting al., 2019). There were two expository and two narrative passages in total, although participants were counterbalanced to read one narrative and one expository passage out of the four. The first expository passage was about the artic circle, and the second was about hot air balloons. The first narrative passage was about a grasshopper, and the second was about a monkey and a cat. All four passages contained 350 words.

The goal of the third study was to understand fundamental characteristics of the learner in relation to the text complexity features required for skilled reading comprehension. There were 142 participants in this dataset, who were between the ages of 10-14 (M=11.78, SD = 1.34). The sample was 54.23% male students, and the reported races were 76.26% White, 12.95% Black/African American, 2.16% Asian, 5.76% more than one race, and 2.88% preferred not to answer. These two expository passages were experimenter-created and were controlled to be as identical as possible based on cohesion, vocabulary, decoding, and syntax (Spencer et al., 2019). The first expository passage was about deserts, and the second was about toads. Both passages contained 305 words. At the time of analysis, the results from the ‘deserts’ passage had not been coded and, thus, were not included.

#### Data compilation. 

From the ~120,000 words, we removed all function words so that only content words remained (398 unique words and 54,943 observations). From there, each TAADA feature for each unique word was added to a table. Each of the unique words in the table was checked to see if there were any NA values for the TAADA variables. The variables related to weighted probability counts (min, average, and max) for consonant, vowel, and all character counts showed NA values for three words (eye, moonless, and seethrough). These words were removed from the data frame leaving 395 unique words and 54,465 observations. We also looked for TAADA variables that showed high zero counts of over 20% of the data (i.e., no reported value for the word). Five TAADA variables showed high zero counts. All variables were related to neighborhood effects (e.g., number of orthographic and phonographic neighbors along with frequency of phonographic and orthographic neighbors) and were removed. The final table included 395 unique words, 54 decoding variables, and 54,465 observations.
	
	The final data frame had a few important considerations. The first is that the data is repeated with each participant providing multiple data points often across more than one passage. The second is that the table showed a strong ceiling effect with 51,287 of the observations showing no miscues and 3,178 observations showing miscues. Lastly, the outcome variable in the table was binary (miscue or no miscue). 

	One concern is that machine learning models trained on the raw data would likely capitalize on the significant imbalanced classification codes reported in the dataset and learn the patterns in the majority class (no miscues) at the expense of the minority class (miscues) because of the distributive properties of the data set. The result would be classification metrics such as precision, recall, and F1 scores that were biased toward the majority class (He & Garcia, 2009). Taking this into consideration, conducted two sets of analyses based on the original imbalanced dataset and on an oversampled dataset that randomly samples observations from the minority class in order to balance samples across the two classes. We selected oversampling versus undersampling because undersampling may cause the classifier to miss important concepts within the over-represented category and because research indicates that oversampling does not suffer from overfitting or other performance degrading effects to the same degree as undersampling (Vanhoeyveld & Martens, 2018). We oversampled using the Random Over-Sampling Examples (ROSE; Lunardon et al., 2014) package in R so that we had 51,287 observations for both miscues and non-miscues. 

#### Statistical Analysis. 

To examine differences in miscued and correctly spoken words based on decoding variables reported in TAADA, we first controlled for multi-collinearity and effect sizes within the data as reported in Study 1 using dummy coded variables for miscued and correctly spoken words. We then scaled the remaining variables and constructed a generalized linear mixed model (GLMM) using the lme4 package in R. A GLMM can include both fixed and random effects along with binomial distributions when developing a classification model (Faraway, 2016). It uses a logit link function to transform probabilities into a log-odds scale in order to model binary outcomes. In our GLMM model, the response variable was each spoken word as a binomial response defined as either correct (coded as 0) or miscued (coded as 1). The fixed effects were the decoding features from TAADA that were not highly correlated. We had two random effects (intercepts). The first controlled for potential variation across participants and the second controlled for potential variation across passages. We ran two sets of GLMMs. The first set focused on the oversampled data and the second set focused on the original data. For the oversampled data, we first created a baseline model that only included random effects. We then developed a full model that included random and fixed effects. All models were checked for suppression effects and hand-pruned if suppression effects were noted. Non-significant predictors were also removed from final models. An ANOVA comparison was made between the baseline and full models to examine differences in strength. Co-efficient estimates along with z scores and p values are reported along with machine learning metrics including precision, recall, F1, and Kappa scores. The second set of GLMMs were based on the original data (i.e., not the oversampled data) using only the meaningful predictor variables from the oversampled GLMM. Like with the oversampled data, we first created a baseline model that only included random effects. We then created a full model that included random and fixed effects. An ANOVA comparison was made between the baseline model and the final model to examine differences in strength. Because of differences in class sizes, no machine learning metrics are reported for this analysis, but co-efficient estimates along with z scores and p values are reported.



